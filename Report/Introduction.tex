\section{Introduction}

Image classification remains at the forefront of machine learning (ML) research with its wide applications in autonomous driving, medical imaging, augmented reality, etc. However, current literature focuses primarily on increasing test accuracy on common benchmark datasets. As state-of-the-art (SOTA) models are released at reduced intervals, it is important to uncover the black box of neural networks and understand the how confident they are in their decisions. Interpretability is critical for AI adoption in sensitive government, business, and medical applications. In this paper, we employ Bayesian Convolutional Neural Networks (BCNN) – a probabilistic CNN architecture – to gain insights into the confidence of neural network predictions on the Fashion MNIST dataset. Bayesian neural networks differ from DNNs in that their weights are assigned a probability distribution instead of a single value/point estimate. These probability distributions describe the uncertainty in weights and can thus be used to estimate uncertainty in predictions. Training a Bayesian neural network via variational inference learns the parameters of these distributions instead of the weights directly via \textit{Bayes by Backprop}. We finally explore the BCNN's efficacy in the tasks of image classification on familiar examples, as well as its diffidence in classifying out-of-dataset images.

